{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Regression Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Mean Absolute Error (MAE):** Measures the average magnitude of the errors in a set of predictions, without considering their direction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Mean Squared Error (MSE):** Measures the average of the squares of the errors. It is more sensitive to outliers than MAE."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Root Mean Squared Error (RMSE):** The square root of the mean squared error. It represents the standard deviation of the residuals."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle RMSE = \\sqrt{MSE}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Max Error:** Measures the maximum residual error."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Max Error} = \\max(|y_i - \\hat{y}_i|)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Coverage Error:** Measures the average number of labels that need to be included in the prediction so that all true labels are predicted."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Coverage Error} = \\frac{1}{n} \\sum_{i=1}^{n} \\min(k : y_i \\in \\text{Top-k}(\\hat{y}_i))$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Mean Absolute Percentage Error (MAPE):** Measures the average of the absolute percentage errors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Median Absolute Error:** Measures the median of the absolute errors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Median Absolute Error} = \\text{median}(|y_i - \\hat{y}_i|)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Mean Squared Logarithmic Error (MSLE):** Measures the mean of the squared logarithmic errors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle MSLE = \\frac{1}{n} \\sum_{i=1}^{n} (\\log(1 + y_i) - \\log(1 + \\hat{y}_i))^2$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Root Mean Squared Logarithmic Error (RMSLE):** The square root of the mean squared logarithmic error."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle RMSLE = \\sqrt{MSLE}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Classification Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Accuracy Score:** Measures the ratio of correctly predicted instances to the total instances."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Accuracy = \\frac{1}{n_{samples}} \\sum_{i=0}^{n_{samples}-1} 1(\\hat{y}_i = y_i)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Precision Score:** Measures the ratio of correctly predicted positive observations to the total predicted positives."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Precision = \\frac{TP}{TP + FP}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Recall Score:** Measures the ratio of correctly predicted positive observations to all observations in the actual class."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Recall = \\frac{TP}{TP + FN}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**F1 Score:** The weighted average of Precision and Recall."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROC AUC Score:** Measures the area under the ROC curve."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{ROC AUC} = \\int_{0}^{1} \\text{ROC}(t) \\, dt$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**R2 Score:** Measures the proportion of the variance in the dependent variable that is predictable from the independent variables."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**D2 Absolute Error Score:** Measures the degree of deviation from the absolute error model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle D^2 = 1 - \\frac{\\sum_{i=1}^{n} |y_i - \\hat{y}_i|}{\\sum_{i=1}^{n} |y_i - \\bar{y}|}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**NDCG Score:** Measures the quality of a ranking."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle NDCG = \\frac{DCG}{IDCG}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rand Score:** Measures the similarity between two data clusterings."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Rand = \\frac{TP + TN}{TP + FP + FN + TN}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**DCG Score:** Measures the ranking quality."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle DCG = \\sum_{i=1}^{n} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Fbeta Score:** The weighted average of Precision and Recall with a weighting factor beta."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle F_\\beta = (1 + \\beta^2) \\times \\frac{Precision \\times Recall}{\\beta^2 \\times Precision + Recall}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Adjusted Rand Score:** Measures the similarity between two data clusterings with adjustment for chance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Adjusted \\, Rand = \\frac{RI - Expected \\, RI}{Max \\, RI - Expected \\, RI}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Silhouette Score:** Measures how similar an object is to its own cluster compared to other clusters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Silhouette = \\frac{b - a}{\\max(a, b)}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Completeness Score:** Measures if all the data points that are members of a given class are elements of the same cluster."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Completeness = 1 - \\frac{H(C|K)}{H(C)}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Homogeneity Score:** Measures if each cluster contains only members of a single class."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Homogeneity = 1 - \\frac{H(K|C)}{H(K)}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Jaccard Score:** Measures similarity between sample sets."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Jaccard = \\frac{|A \\cap B|}{|A \\cup B|}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Consensus Score:** Measures the consensus of clustering results."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Consensus = \\frac{2}{n(n-1)} \\sum_{i < j} \\frac{a_{ij} \\cdot b_{ij}}{\\sqrt{a_{ij} \\cdot a_{ji} \\cdot b_{ij} \\cdot b_{ji}}}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**V-measure Score:** Measures the harmonic mean between Homogeneity and Completeness."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle V = 2 \\times \\frac{Homogeneity \\times Completeness}{Homogeneity + Completeness}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Brier Score Loss:** Measures the mean squared difference between predicted probability and the actual outcome."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Brier = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{p}_i - y_i)^2$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**D2 Tweedie Score:** Measures the proportion of Tweedie deviance explained."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle D^2 = 1 - \\frac{Deviance(y, \\hat{y})}{Deviance(y, \\bar{y})}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Cohen Kappa Score:** Measures the agreement between two raters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\kappa = \\frac{p_o - p_e}{1 - p_e}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**D2 Pinball Score:** Measures the proportion of pinball loss explained."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle D^2 = 1 - \\frac{PinballLoss(y, \\hat{y})}{PinballLoss(y, \\bar{y})}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Mutual Info Score:** Measures the mutual information between two labels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle I(X; Y) = \\sum_{y \\in Y} \\sum_{x \\in X} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Adjusted Mutual Info Score:** Measures the mutual information between two labels with adjustment for chance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle AMI = \\frac{MI - E[MI]}{\\max(H(U), H(V)) - E[MI]}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Average Precision Score:** Measures the area under the precision-recall curve."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle AP = \\sum_n (R_n - R_{n-1}) P_n$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Label Ranking Average Precision Score:** Measures the average precision score for label ranking."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle LRAP = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\sum_{j=1}^{k} P(y_{ij})}{\\max_{y_{ij}} P(y_{ij})}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Balanced Accuracy Score:** Measures the balanced accuracy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Balanced \\, Accuracy = \\frac{Sensitivity + Specificity}{2}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Top K Accuracy Score:** Measures the accuracy considering the top k predictions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Top \\, K \\, Accuracy = \\frac{1}{n} \\sum_{i=1}^{n} 1(y_i \\in \\text{Top-k}(\\hat{y}_i))$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Calinski Harabasz Score:** Measures the ratio of the sum of between-cluster dispersion and within-cluster dispersion."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle CH = \\frac{Tr(B_k)}{Tr(W_k)} \\times \\frac{n - k}{k - 1}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Math\n",
    "\n",
    "# Function to display metric formulas\n",
    "def display_metric(name, description, formula):\n",
    "    display(Markdown(f\"**{name}:** {description}\"))\n",
    "    display(Math(formula))\n",
    "\n",
    "# Regression Metrics\n",
    "display(Markdown(\"## Regression Metrics\"))\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "display_metric(\"Mean Absolute Error (MAE)\",\n",
    "               \"Measures the average magnitude of the errors in a set of predictions, without considering their direction.\",\n",
    "               r'MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|')\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "display_metric(\"Mean Squared Error (MSE)\",\n",
    "               \"Measures the average of the squares of the errors. It is more sensitive to outliers than MAE.\",\n",
    "               r'MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2')\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "display_metric(\"Root Mean Squared Error (RMSE)\",\n",
    "               \"The square root of the mean squared error. It represents the standard deviation of the residuals.\",\n",
    "               r'RMSE = \\sqrt{MSE}')\n",
    "\n",
    "# Max Error\n",
    "display_metric(\"Max Error\",\n",
    "               \"Measures the maximum residual error.\",\n",
    "               r'\\text{Max Error} = \\max(|y_i - \\hat{y}_i|)')\n",
    "\n",
    "# Coverage Error\n",
    "display_metric(\"Coverage Error\",\n",
    "               \"Measures the average number of labels that need to be included in the prediction so that all true labels are predicted.\",\n",
    "               r'\\text{Coverage Error} = \\frac{1}{n} \\sum_{i=1}^{n} \\min(k : y_i \\in \\text{Top-k}(\\hat{y}_i))')\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "display_metric(\"Mean Absolute Percentage Error (MAPE)\",\n",
    "               \"Measures the average of the absolute percentage errors.\",\n",
    "               r'MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100')\n",
    "\n",
    "# Median Absolute Error\n",
    "display_metric(\"Median Absolute Error\",\n",
    "               \"Measures the median of the absolute errors.\",\n",
    "               r'\\text{Median Absolute Error} = \\text{median}(|y_i - \\hat{y}_i|)')\n",
    "\n",
    "# Mean Squared Logarithmic Error (MSLE)\n",
    "display_metric(\"Mean Squared Logarithmic Error (MSLE)\",\n",
    "               \"Measures the mean of the squared logarithmic errors.\",\n",
    "               r'MSLE = \\frac{1}{n} \\sum_{i=1}^{n} (\\log(1 + y_i) - \\log(1 + \\hat{y}_i))^2')\n",
    "\n",
    "# Root Mean Squared Logarithmic Error (RMSLE)\n",
    "display_metric(\"Root Mean Squared Logarithmic Error (RMSLE)\",\n",
    "               \"The square root of the mean squared logarithmic error.\",\n",
    "               r'RMSLE = \\sqrt{MSLE}')\n",
    "\n",
    "# Classification Metrics\n",
    "display(Markdown(\"## Classification Metrics\"))\n",
    "\n",
    "# Accuracy Score\n",
    "display_metric(\"Accuracy Score\",\n",
    "               \"Measures the ratio of correctly predicted instances to the total instances.\",\n",
    "               r'Accuracy = \\frac{1}{n_{samples}} \\sum_{i=0}^{n_{samples}-1} 1(\\hat{y}_i = y_i)')\n",
    "\n",
    "# Precision Score\n",
    "display_metric(\"Precision Score\",\n",
    "               \"Measures the ratio of correctly predicted positive observations to the total predicted positives.\",\n",
    "               r'Precision = \\frac{TP}{TP + FP}')\n",
    "\n",
    "# Recall Score\n",
    "display_metric(\"Recall Score\",\n",
    "               \"Measures the ratio of correctly predicted positive observations to all observations in the actual class.\",\n",
    "               r'Recall = \\frac{TP}{TP + FN}')\n",
    "\n",
    "# F1 Score\n",
    "display_metric(\"F1 Score\",\n",
    "               \"The weighted average of Precision and Recall.\",\n",
    "               r'F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}')\n",
    "\n",
    "# ROC AUC Score\n",
    "display_metric(\"ROC AUC Score\",\n",
    "               \"Measures the area under the ROC curve.\",\n",
    "               r'\\text{ROC AUC} = \\int_{0}^{1} \\text{ROC}(t) \\, dt')\n",
    "\n",
    "# R2 Score\n",
    "display_metric(\"R2 Score\",\n",
    "               \"Measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\",\n",
    "               r'R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}')\n",
    "\n",
    "# D2 Absolute Error Score\n",
    "display_metric(\"D2 Absolute Error Score\",\n",
    "               \"Measures the degree of deviation from the absolute error model.\",\n",
    "               r'D^2 = 1 - \\frac{\\sum_{i=1}^{n} |y_i - \\hat{y}_i|}{\\sum_{i=1}^{n} |y_i - \\bar{y}|}')\n",
    "\n",
    "# NDCG Score\n",
    "display_metric(\"NDCG Score\",\n",
    "               \"Measures the quality of a ranking.\",\n",
    "               r'NDCG = \\frac{DCG}{IDCG}')\n",
    "\n",
    "# Rand Score\n",
    "display_metric(\"Rand Score\",\n",
    "               \"Measures the similarity between two data clusterings.\",\n",
    "               r'Rand = \\frac{TP + TN}{TP + FP + FN + TN}')\n",
    "\n",
    "# DCG Score\n",
    "display_metric(\"DCG Score\",\n",
    "               \"Measures the ranking quality.\",\n",
    "               r'DCG = \\sum_{i=1}^{n} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)}')\n",
    "\n",
    "# Fbeta Score\n",
    "display_metric(\"Fbeta Score\",\n",
    "               \"The weighted average of Precision and Recall with a weighting factor beta.\",\n",
    "               r'F_\\beta = (1 + \\beta^2) \\times \\frac{Precision \\times Recall}{\\beta^2 \\times Precision + Recall}')\n",
    "\n",
    "# Adjusted Rand Score\n",
    "display_metric(\"Adjusted Rand Score\",\n",
    "               \"Measures the similarity between two data clusterings with adjustment for chance.\",\n",
    "               r'Adjusted \\, Rand = \\frac{RI - Expected \\, RI}{Max \\, RI - Expected \\, RI}')\n",
    "\n",
    "# Silhouette Score\n",
    "display_metric(\"Silhouette Score\",\n",
    "               \"Measures how similar an object is to its own cluster compared to other clusters.\",\n",
    "               r'Silhouette = \\frac{b - a}{\\max(a, b)}')\n",
    "\n",
    "# Completeness Score\n",
    "display_metric(\"Completeness Score\",\n",
    "               \"Measures if all the data points that are members of a given class are elements of the same cluster.\",\n",
    "               r'Completeness = 1 - \\frac{H(C|K)}{H(C)}')\n",
    "\n",
    "# Homogeneity Score\n",
    "display_metric(\"Homogeneity Score\",\n",
    "               \"Measures if each cluster contains only members of a single class.\",\n",
    "               r'Homogeneity = 1 - \\frac{H(K|C)}{H(K)}')\n",
    "\n",
    "# Jaccard Score\n",
    "display_metric(\"Jaccard Score\",\n",
    "               \"Measures similarity between sample sets.\",\n",
    "               r'Jaccard = \\frac{|A \\cap B|}{|A \\cup B|}')\n",
    "\n",
    "# Consensus Score\n",
    "display_metric(\"Consensus Score\",\n",
    "               \"Measures the consensus of clustering results.\",\n",
    "               r'Consensus = \\frac{2}{n(n-1)} \\sum_{i < j} \\frac{a_{ij} \\cdot b_{ij}}{\\sqrt{a_{ij} \\cdot a_{ji} \\cdot b_{ij} \\cdot b_{ji}}}')\n",
    "\n",
    "# V-measure Score\n",
    "display_metric(\"V-measure Score\",\n",
    "               \"Measures the harmonic mean between Homogeneity and Completeness.\",\n",
    "               r'V = 2 \\times \\frac{Homogeneity \\times Completeness}{Homogeneity + Completeness}')\n",
    "\n",
    "# Brier Score Loss\n",
    "display_metric(\"Brier Score Loss\",\n",
    "               \"Measures the mean squared difference between predicted probability and the actual outcome.\",\n",
    "               r'Brier = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{p}_i - y_i)^2')\n",
    "\n",
    "# D2 Tweedie Score\n",
    "display_metric(\"D2 Tweedie Score\",\n",
    "               \"Measures the proportion of Tweedie deviance explained.\",\n",
    "               r'D^2 = 1 - \\frac{Deviance(y, \\hat{y})}{Deviance(y, \\bar{y})}')\n",
    "\n",
    "# Cohen Kappa Score\n",
    "display_metric(\"Cohen Kappa Score\",\n",
    "               \"Measures the agreement between two raters.\",\n",
    "               r'\\kappa = \\frac{p_o - p_e}{1 - p_e}')\n",
    "\n",
    "# D2 Pinball Score\n",
    "display_metric(\"D2 Pinball Score\",\n",
    "               \"Measures the proportion of pinball loss explained.\",\n",
    "               r'D^2 = 1 - \\frac{PinballLoss(y, \\hat{y})}{PinballLoss(y, \\bar{y})}')\n",
    "\n",
    "# Mutual Info Score\n",
    "display_metric(\"Mutual Info Score\",\n",
    "               \"Measures the mutual information between two labels.\",\n",
    "               r'I(X; Y) = \\sum_{y \\in Y} \\sum_{x \\in X} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)}')\n",
    "\n",
    "# Adjusted Mutual Info Score\n",
    "display_metric(\"Adjusted Mutual Info Score\",\n",
    "               \"Measures the mutual information between two labels with adjustment for chance.\",\n",
    "               r'AMI = \\frac{MI - E[MI]}{\\max(H(U), H(V)) - E[MI]}')\n",
    "\n",
    "# Average Precision Score\n",
    "display_metric(\"Average Precision Score\",\n",
    "               \"Measures the area under the precision-recall curve.\",\n",
    "               r'AP = \\sum_n (R_n - R_{n-1}) P_n')\n",
    "\n",
    "# Label Ranking Average Precision Score\n",
    "display_metric(\"Label Ranking Average Precision Score\",\n",
    "               \"Measures the average precision score for label ranking.\",\n",
    "               r'LRAP = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\sum_{j=1}^{k} P(y_{ij})}{\\max_{y_{ij}} P(y_{ij})}')\n",
    "\n",
    "# Balanced Accuracy Score\n",
    "display_metric(\"Balanced Accuracy Score\",\n",
    "               \"Measures the balanced accuracy.\",\n",
    "               r'Balanced \\, Accuracy = \\frac{Sensitivity + Specificity}{2}')\n",
    "\n",
    "# Top K Accuracy Score\n",
    "display_metric(\"Top K Accuracy Score\",\n",
    "               \"Measures the accuracy considering the top k predictions.\",\n",
    "               r'Top \\, K \\, Accuracy = \\frac{1}{n} \\sum_{i=1}^{n} 1(y_i \\in \\text{Top-k}(\\hat{y}_i))')\n",
    "\n",
    "# Calinski Harabasz Score\n",
    "display_metric(\"Calinski Harabasz Score\",\n",
    "               \"Measures the ratio of the sum of between-cluster dispersion and within-cluster dispersion.\",\n",
    "               r'CH = \\frac{Tr(B_k)}{Tr(W_k)} \\times \\frac{n - k}{k - 1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iterpreter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
